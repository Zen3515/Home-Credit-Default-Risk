{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "# import os\n",
    "#import seaborn as sns\n",
    "#import category_encoders as ce\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "#from xgboost import XGBClassifier\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_XGB_no_fillnan = pd.read_csv('XGB_FE2_No_fillNaN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48744.000000</td>\n",
       "      <td>48744.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>277796.676350</td>\n",
       "      <td>0.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>103169.547296</td>\n",
       "      <td>0.160662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>100001.000000</td>\n",
       "      <td>0.017061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>188557.750000</td>\n",
       "      <td>0.084904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>277549.000000</td>\n",
       "      <td>0.156068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>367555.500000</td>\n",
       "      <td>0.291792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>456250.000000</td>\n",
       "      <td>0.864964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SK_ID_CURR        TARGET\n",
       "count   48744.000000  48744.000000\n",
       "mean   277796.676350      0.209100\n",
       "std    103169.547296      0.160662\n",
       "min    100001.000000      0.017061\n",
       "25%    188557.750000      0.084904\n",
       "50%    277549.000000      0.156068\n",
       "75%    367555.500000      0.291792\n",
       "max    456250.000000      0.864964"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_XGB_no_fillnan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_logistic_linear = pd.read_csv('log_reg_All_table_PCA_new_FE2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.load('data/train_label.npy')\n",
    "train_PCA = np.load('data/train_PCA.npy')\n",
    "test_PCA = np.load('data/test_PCA.npy')\n",
    "#train_eng = np.load('data/train_eng.npy')\n",
    "#test_eng = np.load('data/test_eng.npy')\n",
    "train_SK_ID_CURR = np.load('data/train_SK_ID_CURR.npy')\n",
    "test_SK_ID_CURR = np.load('data/test_SK_ID_CURR.npy')\n",
    "feature_XGB = pd.read_csv('data/train_XGB_FE2_No_fillNaN2.csv')\n",
    "test_XGB = pd.read_csv('data/test_XGB_FE2_No_fillNaN2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_eng = pd.read_csv('data/Test_Eng2.csv')\n",
    "#test_SK_ID_CURR = test_eng['SK_ID_CURR']\n",
    "#del test_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_eng = pd.read_csv('data/Train_Eng2.csv')\n",
    "#train_SK_ID_CURR = train_eng['SK_ID_CURR']\n",
    "#del train_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/train_SK_ID_CURR', train_SK_ID_CURR)\n",
    "#np.save('data/test_SK_ID_CURR', test_SK_ID_CURR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainFeat = pd.DataFrame(train_PCA)\n",
    "# testFeat = pd.DataFrame(test_PCA)\n",
    "# trainFeat['XGB_no_fillnan'] = feature_XGB_no_fillnan['TARGET']\n",
    "# testFeat['XGB_no_fillnan'] = feature_XGB_no_fillnan['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(train_PCA, train_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PCA = pd.DataFrame(train_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(train_PCA, train_label, test_size=0.2, random_state=42, stratify=train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181648</th>\n",
       "      <td>310536</td>\n",
       "      <td>0.168522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229245</th>\n",
       "      <td>365516</td>\n",
       "      <td>0.166022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122525</th>\n",
       "      <td>242055</td>\n",
       "      <td>0.410851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306311</th>\n",
       "      <td>454894</td>\n",
       "      <td>0.327058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300658</th>\n",
       "      <td>448321</td>\n",
       "      <td>0.527807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201033</th>\n",
       "      <td>333032</td>\n",
       "      <td>0.056296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86445</th>\n",
       "      <td>200322</td>\n",
       "      <td>0.282444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225267</th>\n",
       "      <td>360927</td>\n",
       "      <td>0.145468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42927</th>\n",
       "      <td>149698</td>\n",
       "      <td>0.160273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240820</th>\n",
       "      <td>378855</td>\n",
       "      <td>0.280072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193691</th>\n",
       "      <td>324606</td>\n",
       "      <td>0.594031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190669</th>\n",
       "      <td>321079</td>\n",
       "      <td>0.205331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270875</th>\n",
       "      <td>414014</td>\n",
       "      <td>0.057748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97464</th>\n",
       "      <td>213156</td>\n",
       "      <td>0.114189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283993</th>\n",
       "      <td>428890</td>\n",
       "      <td>0.067247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23750</th>\n",
       "      <td>127633</td>\n",
       "      <td>0.065155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124306</th>\n",
       "      <td>244153</td>\n",
       "      <td>0.338805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28423</th>\n",
       "      <td>133035</td>\n",
       "      <td>0.068531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252183</th>\n",
       "      <td>391796</td>\n",
       "      <td>0.171949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39941</th>\n",
       "      <td>146268</td>\n",
       "      <td>0.200776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115584</th>\n",
       "      <td>234040</td>\n",
       "      <td>0.356156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29456</th>\n",
       "      <td>134201</td>\n",
       "      <td>0.431060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37475</th>\n",
       "      <td>143410</td>\n",
       "      <td>0.169535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148624</th>\n",
       "      <td>272323</td>\n",
       "      <td>0.039713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264008</th>\n",
       "      <td>405722</td>\n",
       "      <td>0.053579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279203</th>\n",
       "      <td>423442</td>\n",
       "      <td>0.190892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108065</th>\n",
       "      <td>225361</td>\n",
       "      <td>0.132829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115746</th>\n",
       "      <td>234224</td>\n",
       "      <td>0.079699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247351</th>\n",
       "      <td>386206</td>\n",
       "      <td>0.470765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>100286</td>\n",
       "      <td>0.158809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60759</th>\n",
       "      <td>170463</td>\n",
       "      <td>0.183287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234644</th>\n",
       "      <td>371805</td>\n",
       "      <td>0.350597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3487</th>\n",
       "      <td>104072</td>\n",
       "      <td>0.164825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88178</th>\n",
       "      <td>202367</td>\n",
       "      <td>0.369925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128969</th>\n",
       "      <td>249593</td>\n",
       "      <td>0.193903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30712</th>\n",
       "      <td>135645</td>\n",
       "      <td>0.157006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279566</th>\n",
       "      <td>423884</td>\n",
       "      <td>0.727164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26201</th>\n",
       "      <td>130474</td>\n",
       "      <td>0.200218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203798</th>\n",
       "      <td>336269</td>\n",
       "      <td>0.554104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177502</th>\n",
       "      <td>305696</td>\n",
       "      <td>0.104367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23286</th>\n",
       "      <td>127090</td>\n",
       "      <td>0.164569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70057</th>\n",
       "      <td>181266</td>\n",
       "      <td>0.245919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258456</th>\n",
       "      <td>399099</td>\n",
       "      <td>0.132032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163675</th>\n",
       "      <td>289735</td>\n",
       "      <td>0.073398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148573</th>\n",
       "      <td>272263</td>\n",
       "      <td>0.083723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304722</th>\n",
       "      <td>453043</td>\n",
       "      <td>0.222307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288987</th>\n",
       "      <td>434796</td>\n",
       "      <td>0.176934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232332</th>\n",
       "      <td>369099</td>\n",
       "      <td>0.210034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58959</th>\n",
       "      <td>168341</td>\n",
       "      <td>0.226429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234985</th>\n",
       "      <td>372198</td>\n",
       "      <td>0.274711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282761</th>\n",
       "      <td>427501</td>\n",
       "      <td>0.209453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269123</th>\n",
       "      <td>411882</td>\n",
       "      <td>0.112362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255356</th>\n",
       "      <td>395471</td>\n",
       "      <td>0.560323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222225</th>\n",
       "      <td>357427</td>\n",
       "      <td>0.636929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170183</th>\n",
       "      <td>297241</td>\n",
       "      <td>0.053438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31304</th>\n",
       "      <td>136325</td>\n",
       "      <td>0.211726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121193</th>\n",
       "      <td>240509</td>\n",
       "      <td>0.151440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248504</th>\n",
       "      <td>387513</td>\n",
       "      <td>0.051499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175469</th>\n",
       "      <td>303331</td>\n",
       "      <td>0.145838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285162</th>\n",
       "      <td>430259</td>\n",
       "      <td>0.307789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246008 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_CURR    TARGET\n",
       "181648      310536  0.168522\n",
       "229245      365516  0.166022\n",
       "122525      242055  0.410851\n",
       "306311      454894  0.327058\n",
       "300658      448321  0.527807\n",
       "201033      333032  0.056296\n",
       "86445       200322  0.282444\n",
       "225267      360927  0.145468\n",
       "42927       149698  0.160273\n",
       "240820      378855  0.280072\n",
       "193691      324606  0.594031\n",
       "190669      321079  0.205331\n",
       "270875      414014  0.057748\n",
       "97464       213156  0.114189\n",
       "283993      428890  0.067247\n",
       "23750       127633  0.065155\n",
       "124306      244153  0.338805\n",
       "28423       133035  0.068531\n",
       "252183      391796  0.171949\n",
       "39941       146268  0.200776\n",
       "115584      234040  0.356156\n",
       "29456       134201  0.431060\n",
       "37475       143410  0.169535\n",
       "148624      272323  0.039713\n",
       "264008      405722  0.053579\n",
       "279203      423442  0.190892\n",
       "108065      225361  0.132829\n",
       "115746      234224  0.079699\n",
       "247351      386206  0.470765\n",
       "246         100286  0.158809\n",
       "...            ...       ...\n",
       "60759       170463  0.183287\n",
       "234644      371805  0.350597\n",
       "3487        104072  0.164825\n",
       "88178       202367  0.369925\n",
       "128969      249593  0.193903\n",
       "30712       135645  0.157006\n",
       "279566      423884  0.727164\n",
       "26201       130474  0.200218\n",
       "203798      336269  0.554104\n",
       "177502      305696  0.104367\n",
       "23286       127090  0.164569\n",
       "70057       181266  0.245919\n",
       "258456      399099  0.132032\n",
       "163675      289735  0.073398\n",
       "148573      272263  0.083723\n",
       "304722      453043  0.222307\n",
       "288987      434796  0.176934\n",
       "232332      369099  0.210034\n",
       "58959       168341  0.226429\n",
       "234985      372198  0.274711\n",
       "282761      427501  0.209453\n",
       "269123      411882  0.112362\n",
       "255356      395471  0.560323\n",
       "222225      357427  0.636929\n",
       "170183      297241  0.053438\n",
       "31304       136325  0.211726\n",
       "121193      240509  0.151440\n",
       "248504      387513  0.051499\n",
       "175469      303331  0.145838\n",
       "285162      430259  0.307789\n",
       "\n",
       "[246008 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_XGB.iloc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (246008, 994)\n",
      "X_test (61503, 994)\n",
      "Y_train (246008,)\n",
      "Y_test (61503,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train', X_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('Y_train', Y_train.shape)\n",
    "print('Y_test', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19860 Trues out of 246008 =  0.08072908198107379 %\n"
     ]
    }
   ],
   "source": [
    "print('There are', np.sum(Y_train), 'Trues out of 246008 = ', np.sum(Y_train)/246008.0, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5439093 , 6.19355488])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54390852, 6.19365559])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight.compute_class_weight('balanced', np.unique(Y_test), Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for Feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def get_feedforward_nn():    \n",
    "    input1 = Input(shape=(994,))    \n",
    "    x = Dense(512, activation='relu')(input1)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 994)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               509440    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 686,665\n",
      "Trainable params: 686,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "# This is called to clear the original model session in order to use TensorBoard\n",
    "K.clear_session()\n",
    "\n",
    "model_ff = get_feedforward_nn()\n",
    "model_ff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ff\n",
      "Train on 246008 samples, validate on 61503 samples\n",
      "Epoch 1/10\n",
      "246008/246008 [==============================] - 2s 9us/step - loss: 0.2860 - val_loss: 0.2443\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24430, saving model to model_ff_nn.h5\n",
      "Epoch 2/10\n",
      "246008/246008 [==============================] - 2s 8us/step - loss: 0.2449 - val_loss: 0.2420\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24430 to 0.24196, saving model to model_ff_nn.h5\n",
      "Epoch 3/10\n",
      "246008/246008 [==============================] - 2s 8us/step - loss: 0.2413 - val_loss: 0.2425\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.24196\n",
      "Epoch 4/10\n",
      "246008/246008 [==============================] - 2s 8us/step - loss: 0.2387 - val_loss: 0.2400\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24196 to 0.24003, saving model to model_ff_nn.h5\n",
      "Epoch 5/10\n",
      "246008/246008 [==============================] - 2s 8us/step - loss: 0.2364 - val_loss: 0.2403\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24003\n",
      "Epoch 6/10\n",
      "246008/246008 [==============================] - 2s 8us/step - loss: 0.2338 - val_loss: 0.2411\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.24003\n",
      "Epoch 7/10\n",
      "246008/246008 [==============================] - 2s 8us/step - loss: 0.2294 - val_loss: 0.2404\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24003\n",
      "Epoch 8/10\n",
      "246008/246008 [==============================] - 2s 8us/step - loss: 0.2276 - val_loss: 0.2409\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24003\n",
      "Epoch 9/10\n",
      "246008/246008 [==============================] - 2s 8us/step - loss: 0.2260 - val_loss: 0.2407\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24003\n",
      "Epoch 10/10\n",
      "246008/246008 [==============================] - 2s 8us/step - loss: 0.2253 - val_loss: 0.2411\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f62f31bf4e0>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau#, TensorBoard\n",
    "\n",
    "print('start training ff')\n",
    "\n",
    "# Path to save model parameters\n",
    "weight_path_model_ff ='model_ff_nn.h5'\n",
    "# Path to write tensorboard\n",
    "tensorboard_path_model_ff = 'Graphs/ff_nn'\n",
    "\n",
    "callbacks_list_model_ff_nn = [\n",
    "    #TensorBoard(log_dir=tensorboard_path_model_ff, histogram_freq=1, write_graph=True, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "            weight_path_model_ff,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.21, patience=2, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "verbose = 1\n",
    "epochs, batch_size = [10,3096]\n",
    "\n",
    "model_ff.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, class_weight='auto',\n",
    "                callbacks=callbacks_list_model_ff_nn, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(features, labels, model):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation data\n",
    "    \"\"\"\n",
    "    return model.evaluate(x=features, y=labels, batch_size=batch_size, verbose=1, sample_weight=None, steps=None)\n",
    "\n",
    "evaluate(X_test, Y_test, model_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = regressor.predict(data[['X']])\n",
    "# prediction = pd.DataFrame(predictions, columns=['predictions']).to_csv('prediction.csv')\n",
    "\n",
    "# Submission dataframe\n",
    "submit = pd.DataFrame(data=test_SK_ID_CURR, columns=['SK_ID_CURR'])\n",
    "submit['TARGET'] = model_ff.predict(test_PCA)\n",
    "\n",
    "# Save the submission to a csv file\n",
    "submit.to_csv('FF_DO_PCA_AUTO_CALSS_WEIGHT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.036999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0.189854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0.066468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0.014913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.198488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100042</td>\n",
       "      <td>0.097491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100057</td>\n",
       "      <td>0.012948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100065</td>\n",
       "      <td>0.058493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100066</td>\n",
       "      <td>0.006691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100067</td>\n",
       "      <td>0.180698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100074</td>\n",
       "      <td>0.024783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100090</td>\n",
       "      <td>0.026442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100091</td>\n",
       "      <td>0.153591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100092</td>\n",
       "      <td>0.043874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100106</td>\n",
       "      <td>0.040667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100107</td>\n",
       "      <td>0.177424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100109</td>\n",
       "      <td>0.061446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100117</td>\n",
       "      <td>0.026264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100128</td>\n",
       "      <td>0.129703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100141</td>\n",
       "      <td>0.055035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100150</td>\n",
       "      <td>0.048483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100168</td>\n",
       "      <td>0.005557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100169</td>\n",
       "      <td>0.031706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100170</td>\n",
       "      <td>0.257390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100171</td>\n",
       "      <td>0.026647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100172</td>\n",
       "      <td>0.188572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>100184</td>\n",
       "      <td>0.096365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>100187</td>\n",
       "      <td>0.041461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100212</td>\n",
       "      <td>0.012027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100222</td>\n",
       "      <td>0.034811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48714</th>\n",
       "      <td>455963</td>\n",
       "      <td>0.007965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48715</th>\n",
       "      <td>455965</td>\n",
       "      <td>0.015144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48716</th>\n",
       "      <td>456007</td>\n",
       "      <td>0.359334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48717</th>\n",
       "      <td>456008</td>\n",
       "      <td>0.005987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48718</th>\n",
       "      <td>456009</td>\n",
       "      <td>0.051815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48719</th>\n",
       "      <td>456010</td>\n",
       "      <td>0.140830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48720</th>\n",
       "      <td>456011</td>\n",
       "      <td>0.030614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48721</th>\n",
       "      <td>456013</td>\n",
       "      <td>0.190263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48722</th>\n",
       "      <td>456028</td>\n",
       "      <td>0.169908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48723</th>\n",
       "      <td>456058</td>\n",
       "      <td>0.087801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48724</th>\n",
       "      <td>456111</td>\n",
       "      <td>0.100955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48725</th>\n",
       "      <td>456114</td>\n",
       "      <td>0.066392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48726</th>\n",
       "      <td>456115</td>\n",
       "      <td>0.021185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48727</th>\n",
       "      <td>456116</td>\n",
       "      <td>0.009038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48728</th>\n",
       "      <td>456119</td>\n",
       "      <td>0.004257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48729</th>\n",
       "      <td>456120</td>\n",
       "      <td>0.092143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48730</th>\n",
       "      <td>456122</td>\n",
       "      <td>0.188695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48731</th>\n",
       "      <td>456123</td>\n",
       "      <td>0.008247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48732</th>\n",
       "      <td>456166</td>\n",
       "      <td>0.090409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48733</th>\n",
       "      <td>456167</td>\n",
       "      <td>0.031684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48734</th>\n",
       "      <td>456168</td>\n",
       "      <td>0.084928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48735</th>\n",
       "      <td>456169</td>\n",
       "      <td>0.071376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48736</th>\n",
       "      <td>456170</td>\n",
       "      <td>0.009368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48737</th>\n",
       "      <td>456189</td>\n",
       "      <td>0.111332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48738</th>\n",
       "      <td>456202</td>\n",
       "      <td>0.088836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48739</th>\n",
       "      <td>456221</td>\n",
       "      <td>0.059844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48740</th>\n",
       "      <td>456222</td>\n",
       "      <td>0.064971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48741</th>\n",
       "      <td>456223</td>\n",
       "      <td>0.022163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48742</th>\n",
       "      <td>456224</td>\n",
       "      <td>0.043332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48743</th>\n",
       "      <td>456250</td>\n",
       "      <td>0.129404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48744 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SK_ID_CURR    TARGET\n",
       "0          100001  0.036999\n",
       "1          100005  0.189854\n",
       "2          100013  0.066468\n",
       "3          100028  0.014913\n",
       "4          100038  0.198488\n",
       "5          100042  0.097491\n",
       "6          100057  0.012948\n",
       "7          100065  0.058493\n",
       "8          100066  0.006691\n",
       "9          100067  0.180698\n",
       "10         100074  0.024783\n",
       "11         100090  0.026442\n",
       "12         100091  0.153591\n",
       "13         100092  0.043874\n",
       "14         100106  0.040667\n",
       "15         100107  0.177424\n",
       "16         100109  0.061446\n",
       "17         100117  0.026264\n",
       "18         100128  0.129703\n",
       "19         100141  0.055035\n",
       "20         100150  0.048483\n",
       "21         100168  0.005557\n",
       "22         100169  0.031706\n",
       "23         100170  0.257390\n",
       "24         100171  0.026647\n",
       "25         100172  0.188572\n",
       "26         100184  0.096365\n",
       "27         100187  0.041461\n",
       "28         100212  0.012027\n",
       "29         100222  0.034811\n",
       "...           ...       ...\n",
       "48714      455963  0.007965\n",
       "48715      455965  0.015144\n",
       "48716      456007  0.359334\n",
       "48717      456008  0.005987\n",
       "48718      456009  0.051815\n",
       "48719      456010  0.140830\n",
       "48720      456011  0.030614\n",
       "48721      456013  0.190263\n",
       "48722      456028  0.169908\n",
       "48723      456058  0.087801\n",
       "48724      456111  0.100955\n",
       "48725      456114  0.066392\n",
       "48726      456115  0.021185\n",
       "48727      456116  0.009038\n",
       "48728      456119  0.004257\n",
       "48729      456120  0.092143\n",
       "48730      456122  0.188695\n",
       "48731      456123  0.008247\n",
       "48732      456166  0.090409\n",
       "48733      456167  0.031684\n",
       "48734      456168  0.084928\n",
       "48735      456169  0.071376\n",
       "48736      456170  0.009368\n",
       "48737      456189  0.111332\n",
       "48738      456202  0.088836\n",
       "48739      456221  0.059844\n",
       "48740      456222  0.064971\n",
       "48741      456223  0.022163\n",
       "48742      456224  0.043332\n",
       "48743      456250  0.129404\n",
       "\n",
       "[48744 rows x 2 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model_ff = get_feedforward_nn()\n",
    "model_ff.load_weights('model_ff_nn(0.77475).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(data=test_SK_ID_CURR, columns=['SK_ID_CURR'])\n",
    "submit['TARGET'] = model_ff.predict(test_PCA)\n",
    "submit.to_csv('data/test_FF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(data=train_SK_ID_CURR, columns=['SK_ID_CURR'])\n",
    "submit['TARGET'] = model_ff.predict(train_PCA)\n",
    "submit.to_csv('data/train_FF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess data for cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn = X_train.values.reshape((-1,994,1))\n",
    "X_test_cnn = X_test.values.reshape((-1,994,1))\n",
    "X_train_feat_xgb = feature_XGB.iloc[X_train.index]['TARGET'].values\n",
    "X_test_feat_xgb = feature_XGB.iloc[X_test.index]['TARGET'].values\n",
    "del X_train\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout, Conv1D, concatenate, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "#keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', \n",
    "#dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', \n",
    "#bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "#kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "def get_cnn():    \n",
    "    input1 = Input(shape=(994, 1,), name='main_input')\n",
    "    \n",
    "    cv1 = Conv1D(filters=25, kernel_size=3, strides=1, activation='relu')(input1)\n",
    "    cv1 = Dropout(0.25)(cv1)\n",
    "    cv1 = Dense(60, activation='softmax')(cv1)\n",
    "    cv1 = Conv1D(filters=25, kernel_size=3, strides=1, activation='relu')(cv1)\n",
    "    cv1 = Dropout(0.25)(cv1)\n",
    "    cv1 = Dense(10, activation='softmax')(cv1)\n",
    "    cv1 = Flatten()(cv1)\n",
    "    cv1 = Dense(200, activation='relu')(cv1)\n",
    "    \n",
    "    #cv2 = Conv1D(filters=120, kernel_size=3, strides=2, activation='relu')(input1)\n",
    "    #cv2 = Dropout(0.25)(cv2)\n",
    "    #cv2 = Dense(30, activation='softmax')(cv2)\n",
    "    #cv2 = Flatten()(cv2)\n",
    "    \n",
    "    #cv3 = Conv1D(filters=100, kernel_size=3, strides=3, activation='relu')(input1)\n",
    "    #cv3 = Dropout(0.25)(cv3)\n",
    "    #cv3 = Dense(50, activation='softmax')(cv3)\n",
    "    #cv3 = Flatten()(cv3)\n",
    "    \n",
    "    #cv4 = Conv1D(filters=70, kernel_size=3, strides=4, activation='relu')(input1)\n",
    "    #cv4 = Dropout(0.25)(cv4)\n",
    "    #cv4 = Dense(50, activation='softmax')(cv4)\n",
    "    #cv4 = Flatten()(cv4)\n",
    "    \n",
    "    #cv5 = Conv1D(filters=50, kernel_size=3, strides=5, activation='relu')(input1)\n",
    "    #cv5 = Dropout(0.25)(cv5)\n",
    "    #cv5 = Dense(25, activation='softmax')(cv5)\n",
    "    #cv5 = Flatten()(cv5)\n",
    "    \n",
    "    flatinput1 = Flatten()(input1)\n",
    "    \n",
    "    x = concatenate([flatinput1, cv1])\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    xgb_pred = Input(shape=(1,), name='xgb_input')\n",
    "    x = concatenate([xgb_pred, x])\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input1, xgb_pred], outputs=out)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 994, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 992, 25)      100         main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 992, 25)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 992, 60)      1560        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 990, 25)      4525        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 990, 25)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 990, 10)      260         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9900)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 994)          0           main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          1980200     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1194)         0           flatten_2[0][0]                  \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          611840      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          131328      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          32896       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "xgb_input (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          12900       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 101)          0           xgb_input[0][0]                  \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            102         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,775,711\n",
      "Trainable params: 2,775,711\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "# This is called to clear the original model session in order to use TensorBoard\n",
    "K.clear_session()\n",
    "\n",
    "model_cnn = get_cnn()\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training cnn\n",
      "Train on 246008 samples, validate on 61503 samples\n",
      "Epoch 1/9\n",
      "246008/246008 [==============================] - 39s 160us/step - loss: 0.2779 - val_loss: 0.2466\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24665, saving model to model_cnn_ff_xgb.h5\n",
      "Epoch 2/9\n",
      "246008/246008 [==============================] - 38s 154us/step - loss: 0.2482 - val_loss: 0.2453\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24665 to 0.24528, saving model to model_cnn_ff_xgb.h5\n",
      "Epoch 3/9\n",
      "246008/246008 [==============================] - 38s 154us/step - loss: 0.2458 - val_loss: 0.2516\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.24528\n",
      "Epoch 4/9\n",
      "246008/246008 [==============================] - 38s 154us/step - loss: 0.2430 - val_loss: 0.2446\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24528 to 0.24456, saving model to model_cnn_ff_xgb.h5\n",
      "Epoch 5/9\n",
      "246008/246008 [==============================] - 38s 154us/step - loss: 0.2407 - val_loss: 0.2425\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.24456 to 0.24251, saving model to model_cnn_ff_xgb.h5\n",
      "Epoch 6/9\n",
      "246008/246008 [==============================] - 38s 154us/step - loss: 0.2383 - val_loss: 0.2437\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.24251\n",
      "Epoch 7/9\n",
      "246008/246008 [==============================] - 38s 154us/step - loss: 0.2353 - val_loss: 0.2428\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24251\n",
      "Epoch 8/9\n",
      "246008/246008 [==============================] - 38s 154us/step - loss: 0.2302 - val_loss: 0.2403\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.24251 to 0.24027, saving model to model_cnn_ff_xgb.h5\n",
      "Epoch 9/9\n",
      "246008/246008 [==============================] - 38s 155us/step - loss: 0.2280 - val_loss: 0.2408\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae40367cf8>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau#, TensorBoard\n",
    "\n",
    "print('start training cnn')\n",
    "\n",
    "# Path to save model parameters\n",
    "weight_path_model_cnn ='model_cnn_ff_xgb.h5'\n",
    "# Path to write tensorboard\n",
    "tensorboard_path_model_cnn = 'Graphs/cnn'\n",
    "\n",
    "callbacks_list_model_cnn = [\n",
    "    #TensorBoard(log_dir=tensorboard_path_model_cnn, histogram_freq=1, write_graph=True, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "            weight_path_model_cnn,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.21, patience=2, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "verbose = 1\n",
    "epochs, batch_size = [9,3096]\n",
    "\n",
    "model_cnn.fit([X_train_cnn, X_train_feat_xgb.values], Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, class_weight='auto',\n",
    "                callbacks=callbacks_list_model_cnn, validation_data=([X_test_cnn, X_test_feat_xgb], Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model_cnn = get_cnn()\n",
    "model_cnn.load_weights(weight_path_model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(data=test_SK_ID_CURR, columns=['SK_ID_CURR'])\n",
    "submit['TARGET'] = model_cnn.predict([test_PCA.reshape((-1,994,1)), test_XGB['TARGET']])\n",
    "submit.to_csv('cnn_ff_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.041532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0.201485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0.058560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0.015930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.185706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100042</td>\n",
       "      <td>0.135859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100057</td>\n",
       "      <td>0.018926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100065</td>\n",
       "      <td>0.053272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100066</td>\n",
       "      <td>0.006577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100067</td>\n",
       "      <td>0.251378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100074</td>\n",
       "      <td>0.036552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100090</td>\n",
       "      <td>0.045995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100091</td>\n",
       "      <td>0.156645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100092</td>\n",
       "      <td>0.055838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100106</td>\n",
       "      <td>0.046730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100107</td>\n",
       "      <td>0.181262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100109</td>\n",
       "      <td>0.042009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100117</td>\n",
       "      <td>0.034921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100128</td>\n",
       "      <td>0.103693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100141</td>\n",
       "      <td>0.056661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100150</td>\n",
       "      <td>0.044093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100168</td>\n",
       "      <td>0.007169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100169</td>\n",
       "      <td>0.069756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100170</td>\n",
       "      <td>0.179794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100171</td>\n",
       "      <td>0.027650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100172</td>\n",
       "      <td>0.151977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>100184</td>\n",
       "      <td>0.110431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>100187</td>\n",
       "      <td>0.057555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100212</td>\n",
       "      <td>0.017935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100222</td>\n",
       "      <td>0.046471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48714</th>\n",
       "      <td>455963</td>\n",
       "      <td>0.012681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48715</th>\n",
       "      <td>455965</td>\n",
       "      <td>0.018950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48716</th>\n",
       "      <td>456007</td>\n",
       "      <td>0.386231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48717</th>\n",
       "      <td>456008</td>\n",
       "      <td>0.011525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48718</th>\n",
       "      <td>456009</td>\n",
       "      <td>0.048085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48719</th>\n",
       "      <td>456010</td>\n",
       "      <td>0.111044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48720</th>\n",
       "      <td>456011</td>\n",
       "      <td>0.048283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48721</th>\n",
       "      <td>456013</td>\n",
       "      <td>0.215730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48722</th>\n",
       "      <td>456028</td>\n",
       "      <td>0.165249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48723</th>\n",
       "      <td>456058</td>\n",
       "      <td>0.092231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48724</th>\n",
       "      <td>456111</td>\n",
       "      <td>0.104530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48725</th>\n",
       "      <td>456114</td>\n",
       "      <td>0.047505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48726</th>\n",
       "      <td>456115</td>\n",
       "      <td>0.025728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48727</th>\n",
       "      <td>456116</td>\n",
       "      <td>0.010117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48728</th>\n",
       "      <td>456119</td>\n",
       "      <td>0.009381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48729</th>\n",
       "      <td>456120</td>\n",
       "      <td>0.102975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48730</th>\n",
       "      <td>456122</td>\n",
       "      <td>0.139171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48731</th>\n",
       "      <td>456123</td>\n",
       "      <td>0.010073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48732</th>\n",
       "      <td>456166</td>\n",
       "      <td>0.105371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48733</th>\n",
       "      <td>456167</td>\n",
       "      <td>0.054279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48734</th>\n",
       "      <td>456168</td>\n",
       "      <td>0.078993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48735</th>\n",
       "      <td>456169</td>\n",
       "      <td>0.067696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48736</th>\n",
       "      <td>456170</td>\n",
       "      <td>0.014719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48737</th>\n",
       "      <td>456189</td>\n",
       "      <td>0.099581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48738</th>\n",
       "      <td>456202</td>\n",
       "      <td>0.103309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48739</th>\n",
       "      <td>456221</td>\n",
       "      <td>0.044676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48740</th>\n",
       "      <td>456222</td>\n",
       "      <td>0.076956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48741</th>\n",
       "      <td>456223</td>\n",
       "      <td>0.031495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48742</th>\n",
       "      <td>456224</td>\n",
       "      <td>0.043955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48743</th>\n",
       "      <td>456250</td>\n",
       "      <td>0.138713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48744 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SK_ID_CURR    TARGET\n",
       "0          100001  0.041532\n",
       "1          100005  0.201485\n",
       "2          100013  0.058560\n",
       "3          100028  0.015930\n",
       "4          100038  0.185706\n",
       "5          100042  0.135859\n",
       "6          100057  0.018926\n",
       "7          100065  0.053272\n",
       "8          100066  0.006577\n",
       "9          100067  0.251378\n",
       "10         100074  0.036552\n",
       "11         100090  0.045995\n",
       "12         100091  0.156645\n",
       "13         100092  0.055838\n",
       "14         100106  0.046730\n",
       "15         100107  0.181262\n",
       "16         100109  0.042009\n",
       "17         100117  0.034921\n",
       "18         100128  0.103693\n",
       "19         100141  0.056661\n",
       "20         100150  0.044093\n",
       "21         100168  0.007169\n",
       "22         100169  0.069756\n",
       "23         100170  0.179794\n",
       "24         100171  0.027650\n",
       "25         100172  0.151977\n",
       "26         100184  0.110431\n",
       "27         100187  0.057555\n",
       "28         100212  0.017935\n",
       "29         100222  0.046471\n",
       "...           ...       ...\n",
       "48714      455963  0.012681\n",
       "48715      455965  0.018950\n",
       "48716      456007  0.386231\n",
       "48717      456008  0.011525\n",
       "48718      456009  0.048085\n",
       "48719      456010  0.111044\n",
       "48720      456011  0.048283\n",
       "48721      456013  0.215730\n",
       "48722      456028  0.165249\n",
       "48723      456058  0.092231\n",
       "48724      456111  0.104530\n",
       "48725      456114  0.047505\n",
       "48726      456115  0.025728\n",
       "48727      456116  0.010117\n",
       "48728      456119  0.009381\n",
       "48729      456120  0.102975\n",
       "48730      456122  0.139171\n",
       "48731      456123  0.010073\n",
       "48732      456166  0.105371\n",
       "48733      456167  0.054279\n",
       "48734      456168  0.078993\n",
       "48735      456169  0.067696\n",
       "48736      456170  0.014719\n",
       "48737      456189  0.099581\n",
       "48738      456202  0.103309\n",
       "48739      456221  0.044676\n",
       "48740      456222  0.076956\n",
       "48741      456223  0.031495\n",
       "48742      456224  0.043955\n",
       "48743      456250  0.138713\n",
       "\n",
       "[48744 rows x 2 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(data=train_SK_ID_CURR, columns=['SK_ID_CURR'])\n",
    "submit['TARGET'] = model_cnn.predict(train_PCA.reshape((-1,994,1)))\n",
    "submit.to_csv('data/train_cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(data=test_SK_ID_CURR, columns=['SK_ID_CURR'])\n",
    "submit['TARGET'] = model_cnn.predict(test_PCA.reshape((-1,994,1)))\n",
    "submit.to_csv('data/test_cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
